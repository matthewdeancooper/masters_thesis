\chapter{Results and Discussion}
\label{ch:results}

\section{Model 1: Pelvic imaging}
A total of 5 loss functions were assessed for ability to train a 2D U-Net with a small dataset. As seen in Table \ref{table:loss_prostate}, dice similarity coefficient (DSC), precision and sensitivity metrics were recorded on an independent test dataset to measure model generalisability for each loss. 

Binary cross entropy (BCE) achieved the strongest scores with a DSC of $>0.99$, precision of $>0.99$, and sensitivity of $>0.99$. Patient contours had strong agreement under BCE with an average organ specific DSC of $> 0.99$; however, BCE failed to produce any positive predictions for the rectum, and only relatively large bladder examples were identified - likely due to the built-in assumption that classes are balanced \cite{Ronneberger_2015}, resulting in a local minima trapping where optimisation occurred only for the patient \cite{Khan2019}. However, relatively large bladder examples in the test dataset received DSC scores higher than the same contours under w. soft DSC. (see Figures \ref{fig:prostate_bladder} and \ref{fig:prostate_BCE_bladder}).  

The standard soft DSC loss function also assumes equally weighted segmentation classes throughout the data \cite{Sudre_2017}; and hence, failed to produce positive predictions for the bladder and rectum. However, patient contouring had a mean DSC $> 0.99$. Research has indicated that the DSC is equivalent to the harmonic mean of recall (sensitivity) and precision \cite{Bebis2019}; and hence, weighs both equally \cite{Bebis2019} - contributing to the class imbalance problem as the majority of output pixels are negatives. It is not possible to control the trade-off between false-positives and false-negatives directly within the standard soft DSC loss formulation \cite{taghanaki2018};
as expected, we observe a bias that favours negatives due to their over-representation in model output \cite{taghanaki2018}. To control for class imbalance in pelvic CT imaging (where boundaries between OARs can be poorly defined \cite{Liu_2020}) a loss function that enforces a higher penalty for false-negative values may reduce under-segmentation and improve identification in relatively smaller OAR examples \cite{taghanaki2018}. An additional point of warning to note is that the soft DSC does not include true-negatives in its calculation, hence specificity was not optimised directly \cite{taghanaki2018}.

Conversely, the weighted soft DSC was the only loss attempted that was able to optimise for all organs in the contour space (see Figures \ref{fig:prostate_patient}, \ref{fig:prostate_bladder}, and \ref{fig:prostate_rectum}), and was selected as the final model. A simplified combination BCE and weighted soft DSC loss was also attempted (see \cite{taghanaki2018}) after experiments revealed BCE performance was superior to the standard soft DSC metric, and contoured relatively large bladder examples more accurately (i.e. higher DSC and sDSC) than the weighted soft DSC. However, only patient contours were produced under the combination loss. Reinforcing that scalar selection to optimally balance a linear combination of loss functions, is a non-trivial task dependent on the data distribution \cite{Bertels2019}. 
 
Focal Tversky loss (see \cite{Khan2019}) exceeded both BCE and the combination loss in sensitivity; however, performed poorly on average volumetric overlap (DSC). Tversky loss successfully identified rectum and bladder contours in almost all cases; however, segmentation masks contained many false-positive results, with additional groupings that neither resembled nor were spatially close to the OAR in question. Although Tversky loss aims to improve the trade-off between sensitivity and precision compared to DSC for highly imbalanced data (i.e. by weighting to penalise false-negatives higher than false-positives) \cite{taghanaki2018}; our results in Table \ref{table:loss_prostate} indicated sensitivity was lower when compared to BCE and the weighted soft DSC loss. However, Focal Tversky was the only loss function to perform higher in sensitivity than precision - consistent with a higher weighting on false-negatives as described in the literature \cite{Khan2019}.

\input{tables/loss_prostate}

The final model for pelvic imaging was selected at the 140th epoch, under weighted soft DSC. As seen in Figure \ref{fig:prostate_metrics}, validation loss plateaued at 130 epochs, with DSC value reaching a maximum at 140 epochs. Although a smoothed representation of loss for both validation and training data would be monotonically decreasing, this is not the case for DSC, precision, and sensitivity metrics. Section \ref{ch:method-architecture} discussed the use of BCE weight initialisation; hence, after switching from BCE to weighted soft DSC, there is a significant change in loss topology. Examination of early model predictions after BCE initialisation indicated a change in state from placing minimal significance on bladder and rectum cases, to now having gradients dominated by changes to these organs under weighted soft DSC. It is likely that feature representation were subsequently perturbed significantly to minimise the new loss - temporarily decreasing DSC and precision. A similar pattern emerged for DSC value under Focal Tversky; however, the initial DSC value is comparatively lower than under weighted soft DSC - indicating that convergence after three epochs of BCE is sensitive to randomly sampled weight initialisation \cite{Ronneberger_2015}. Total inference time per patient was approximately 3 seconds.

\begin{figure}[H]
	\begin{center}
		\hspace*{-1.2cm}\includegraphics[width=1.15\textwidth]{figures/prostate_metrics_all}
		\caption{\textbf{A)} Model training metrics for pelvic imaging via weighted soft DSC loss (w. soft DSC). Final model selected at epoch 140 due to validation loss plateau. Metrics begin post binary cross entropy (BCE) weight initialisation (3 epochs). Training time of 9~hours. \\
		% I changed 9 hours --> 9~hours. This makes it so that the 9 and the hours are never placed on separate lines.
		\textbf{B)} Soft dice similarity coefficient (soft DSC) loss \\
		\textbf{C)} Combination binary cross entropy (BCE) and weighted soft dice similarity coefficient (w. soft DSC) loss\\
		\textbf{D)}  Focal Tversky loss}
		\label{fig:prostate_metrics}
	\end{center}
\end{figure}


\begin{figure}[H]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{figures/prostate_patient}
		\caption{Representative output for patient. Model 1 - trained via weighted soft dice (w. soft DSC) loss - 140 epochs.  Truth contour (yellow), prediction contour (red). Mean surface distance (MSD) mm.}
		\label{fig:prostate_patient}
	\end{center}
\end{figure}



\begin{figure}[H]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{figures/prostate_bladder}
		\caption{Representative output for bladder: Model 1 - trained via weighted soft dice (w. soft DSC) loss - 140 epochs. Truth contour (yellow), prediction contour (red). Mean surface distance (MSD) in mm. sDSC \cite{Nikolov_2018} calculated at an organ specific tolerance of $\tau$ = 1.46 mm, the 95th percentile mean surface distance between expert observers \cite{Roach_2019}.}
		\label{fig:prostate_bladder}
	\end{center}
\end{figure}

In Figure \ref{fig:prostate_bladder}, weighted soft DSC underestimated the posterior aspect of larger bladder examples - a recurrent limitation of the model. Figure \ref{fig:prostate_bladder} C) shows the model was correctly able to identify when contours were not present, indicating the strong negative predictive validity of the model - quantified by the high sensitivity score recorded on Table \ref{table:organ}.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{figures/prostate_BCE_bladder}
		\caption{Representative model output for bladder: Trained via binary cross entropy loss - 78 epochs. Truth contour (yellow), prediction contour (red). Mean surface distance (MSD) in mm. sDSC \cite{Nikolov_2018} calculated at an organ specific tolerance of $\tau$ = 1.46 mm, the 95th percentile mean surface distance between expert observers \cite{Roach_2019}.}
		\label{fig:prostate_BCE_bladder}
	\end{center}
\end{figure}

Model 1 under weighted soft DSC was unable to identify rectum contours containing hollow regions (18\% of rectum contours) as seen in Figure \ref{fig:prostate_rectum} D. It is suspected that further training on a distribution of similar cases may improve performance. All other OAR examples in the test dataset were correctly identified. As seen in Figure \ref{fig:prostate_rectum} and Table \ref{table:organ}, DSC values for rectum contours were lower on average when compared to the bladder - consistent with expert IOV \cite{Roach_2019} and other models in the literature \cite{Liu_2020, Kazemifar_2018, Wong2020}. However, a higher sDSC indicated that the degree of correction required for the identified rectum contours was lower than the correction required for bladder contours.



\begin{figure}[H]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{figures/prostate_rectum}
		\caption{Representative output for rectum: Model 1 - trained via weighted soft dice (w. soft DSC) loss - 140 epochs. Truth contour (yellow), prediction contour (red). Mean surface distance (MSD) in mm. sDSC \cite{Nikolov_2018} calculated at an organ specific tolerance of $\tau$ = 6.99 mm, the 95th percentile mean surface distance between expert observers \cite{Roach_2019}.}
		\label{fig:prostate_rectum}
	\end{center}
\end{figure}

\section{Model 2: Canine imaging}

Three loss functions were attempted for vacuum bag segmentation in canine imaging, as seen in Table \ref{table:loss_vet}. Soft DSC outperformed both BCE and focal Tversky on DSC and precision values. Focal Tversky had the highest sensitivity (0.97) as expected \cite{Khan2019}, with BCE second (0.95). The final model was selected at 100 epochs under soft DSC loss. Representative model output is presented in Figure \ref{fig:vet_vacbag}. Soft DSC loss showed excellent agreement with ground truth vacuum bag contours - and was able to handle both negative (E) and small contour (B) examples in the test dataset (see Figure \ref{fig:vet_vacbag}).

\input{tables/loss_vet}

\begin{figure}[H]
	\begin{center}
		\hspace*{-1.2cm}\includegraphics[width=1.15\textwidth]{figures/vacbag_metrics_combined}
		\caption{\textbf{A)} Model training metrics for canine imaging via soft dice similarity coefficient (soft DSC) loss. Final model selected at epoch 100 due to validation loss plateau. Training time 6 hours.\\
		\textbf{B)} Binary cross entropy (BCE) loss\\
		\textbf{C)} Focal Tversky loss}
		\label{fig:vet_metrics}
	\end{center}
\end{figure}

Typically, it is expected that training metrics will overstate a model's predictive capacity, as parameter values are updated to fit the training data distribution. However, Figure \ref{fig:vet_metrics} indicated that at many stages, validation loss was lower than training loss. The literature states two possible contributing factors: 1) Dropout layers regularise only on the training data, and hence the full architecture is only available for inference on the validation and testing sets \cite{srivastava2014}. Additionally, batch normalisation parameters are tuned to normalise activations on the test dataset - and are fixed during validation and testing \cite{santurkar2018}. 2) Due to the small dataset used in this study, variation in the validation set may itself be small compared to the training data; hence, if the validation distribution is centred about the mean of the training data, the validation dataset would be relatively `easier' to infer \cite{Bishop}. Although Tversky loss continued to decrease for both the training and validation sets over the epochs tested, a DSC validation plateau (see oscillation in Figure \ref{fig:vet_metrics} C)) triggered early stopping.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{figures/vet_vacbag}
		\caption{Representative output for vacuum bag: Model 2 - trained via soft DSC loss. Truth contour (yellow), prediction contour (red). Mean surface distance (MSD) mm.}
		\label{fig:vet_vacbag}
	\end{center}
\end{figure}

\section{Clinical relevance}
\todo[inline]{I'm looking for magical words like here are the numbers, and these parameters are always equal to or better contours than the parameters of contours produced by experts. Or something to that extent. The word ``excellent'' doesn't belong here.}

The pelvic imaging model showed excellent agreement with observers for patient contours, with an average DSC value of $>0.99$, and a mean surface distance (MSD) of $<0.1$ mm, as seen in Table \ref{table:organ}. Bladder contours had a DSC score of 0.86 with a STD of 0.2. Additionally, bladder MSD was measured to be 1.1 mm with an STD of 3 mm. Volumetric rectum agreement was considerably lower, with a DSC of 0.67 and STD 0.1, and a MSD of 1.1 mm with STD 2 mm.

\todo[inline]{ Report maximum too}

In comparison, organ specific tolerances used for these OARs (95th percentile MSD between experts - i.e. top 95\% expert variance) were determined to be 1.5 mm and 7.0 mm
for the bladder and rectum, respectively \cite{Roach_2019, Nikolov_2018}. Large variances in bladder MSD corresponded to predictions that under-segmented the posterior aspect of the bladder (as seen in Figure \ref{fig:prostate_bladder} A-B) However, the vast majority of bladder surfaces were contoured correctly within expert IOV - with a mean sDSC 0.87(0.1). In comparison, rectum contours that were correctly identified had a considerable higher number of surface points that did not need correction to be within expert IOV, with a sDSC of 0.92(0.1) recorded.

\todo[inline]{clinically acceptable?}

However, experts have been reported to achieve similarities much higher than this,  DSC 0.93 $\pm$ 0.03, MSD 0.99(0.30) mm for the bladder and DSC 0.81 $\pm$ 0.07, MSD 2.862(2.066) mm for the rectum \cite{Roach_2019}. These finding indicate that although bladder and rectum contours produced by the  may be clinically acceptable, performance falls short of expert IOV.

State-of-the-art U-Net implementations have recently been able to achieve DSC values of 0.95 and 0.92 for the bladder and rectum, respectively \cite{Kazemifar_2018}. However, 85 CT patient scans were included in this dataset, compared to the 16 included in our study. We suspect that increasing the number of patients in our study would lead to higher generalisability in model performance (reported to scale logarithmically with dataset size \cite{Nemoto_2020}) as well as provide a broader validation and test distribution - improving the reliability and robustness of performance metrics \cite{Ronneberger_2015}. 

Model 2 produced vacuum bag contours with an average DSC of 0.95 and MSD of 0.18 mm with STD of 0.275 mm. The vacuum bag material has an electron density of approximately 0.1\% of water \cite{CITATION}; hence, the 95th percentile vacuum bag MSD (0.726 mm) corresponds to a negligible shift is

\todo[inline]{wording}

dose distribution under contours produced by this model - and a potential time saving of 30 minutes per patient. Dose shift has yet to be assessed  in clinic - however, we suggest that comparing dose volume histograms between model and expert contours under an identical treatment plan could provide a quantitative measure of clinical acceptability.

\todo[inline]{This isn't true for vacbag or patient contours. Instead its effect on PDD on the other contoured organs is actually the issue. We don't care about the DVH to the vac bag, and although we do care about the DVH of a patient contour it would be negligably impacted by surface deviations of even 1 - 2 cm due to the shear volume occupied by the patient. However, a 2 cm shift in surface of the patient corresponds to a 10\% dose difference on the following PDD http://ozradonc.wikidot.com/descriptors-of-dose-distribution-photons (60 / 67)}

If acceptance testing validates vacuum bag segmentation, model 2 has the potential to save approximately 30 minutes in treatment planning time per patients.

\input{tables/metrics}


\todo[inline]{To assess properly doesn't need to be a project unto itself, just a figure with a PDD, and a note regarding what a change in patient surface by the largest amount you found in your model corresponds to as a change in dose at 10 cm depth in the patient. It's important, and I suspect you'll lose marks without it.}
	
%One certainly could do that. But you wouldn't need to. Given 2 cm of vacbag is equivalent to 2 mm of water, that would correspond to a dose change of 1\% at depth. You would need to be wrong by 2 cm to get a 1% dose difference. Using your numbers:
%
%MSD of 0.175 mm with STD of 0.275 mm
%
%A 95\% confidence interval (assuming a normal distribution with that STD) puts you at 0.2 mm + 2 * 0.3 mm = 0.8 mm. So, for the greater majority of ray lines I would not expect the dose deviation to be any more than 0.05%.
%
%Combine that with the requirement that a human checks them, specifically narrowing in on the potentially not impossible case where a vac bag is not contoured at all, then they can fix any issues introduced by the model.}



\section{Limitations and future work}

A common challenge in deep-learning applications to medical imaging is small data set sizes \cite{Ronneberger_2015}. Limited data reduces model generalisability \cite{Shen2017}. With state-of-the-art implementations using up to 1000 patients per study \cite{Nikolov_2018}, it is expected that bladder and rectum segmentation could improve significantly with the broader distribution provided by a larger dataset.

Additionally, organ-specific tolerances used in this study were acquired by Roach et al. from 15 expert observers (9 of which were radiation oncologists) averaged over a cohort of 5 patients \cite{Roach_2019}. Reliability in these values could be improved by surveying a broader range of experts and patients.

Only 1 attempt to correlate sDSC with the time required for contour correction was found in a search of all papers including the term `surface dice similarity coefficient' on PubMed. Therefore, more work is required to assess the utility of sDSC in a clinical workflow. Alternative surface-based metrics have also been presented; for example, the estimated added path length in Vaassen et al. (seen in Figure \ref{fig:vaassen}) \cite{Vaassen_2020}. However, Vaassen et al. compared sDSC and estimated added path length under an x-y voxel-size tolerance, rather than under organ specific IOV tolerances \cite{Vaassen_2020}. 

Additionally, there is an opportunity to investigate the barriers and limitations in designing a soft surrogate sDSC metric that can be optimised directly during training.

Furthermore, our ultimate responsibility lies in improving patient outcomes. Hence, there is an opportunity to correlate DSC and sDSC performance with changes to dose distribution when compared with plans developed under expert contouring. A potential advantage of sDSC compared to DSC is the stronger correlation with the time required for contour correction - however, correlation with dose shift may also be an important clinical indicator.

Current studies are researching potential improvements in medical imaging segmentation under 3D U-Net models. Although this study focused on a 2D implementation due to the clinical barriers inherent in 3D models, more work is still required to quantify the potential for performance improvement. 

